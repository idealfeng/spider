---
title: 爬虫 Demo 学习文档（配套本仓库）
tags: [爬虫, Python, requests, urllib, bs4, lxml, selenium, scrapy]
created: 2026-02-15
updated: 2026-02-15
---

# 1. 这是什么项目？怎么用它学习

这个仓库更像是“爬虫学习资料夹”：用 **`urllib` / `requests` / `BeautifulSoup` / `lxml(xpath)` / `jsonpath` / `selenium` / `scrapy`** 把常见抓取场景都跑了一遍，并配了部分本地样例文件（HTML/JSON/图片）。

你可以把它当作一条学习路线：

1. **先理解 HTTP 抓取（urllib / requests）** → 会发请求、会带参数、会看响应、会处理编码和异常  
2. **再学解析（xpath / bs4 / json/jsonpath）** → 从响应里提取结构化数据  
3. **再学反爬常识（UA/Referer/Cookie/代理/限速/重试）** → 让脚本更稳、更像“工程”  
4. **遇到动态页面再上浏览器自动化（selenium）**  
5. **最后用 Scrapy 工程化** → 多 spider、管道、下载、持久化、并发、调度

> [!warning] 合规与边界（务必先读）
> - 抓取前先看目标网站的 **robots/服务条款/版权声明**，尤其是登录态、付费内容、个人信息、频繁访问。
> - 仅抓取你有权使用的数据；不要绕过验证码/风控去获取敏感数据。
> - 仓库里有“cookie 登录/表单登录/翻译接口签名”等示例，这类示例很容易**过时**，也可能触发风控；学习思路即可。

---

# 2. 项目结构速览（对照学习）

仓库根目录下主要代码在 `爬虫/`：

- `爬虫/*.py`：单文件脚本，覆盖 urllib / requests / 解析 / selenium 等
- `爬虫/bs4_test.html`、`爬虫/urllib_解析_path.html`：本地 HTML 示例
- `爬虫/jsonpath_本地文件.json`、`爬虫/city.json`：本地 JSON 示例
- `爬虫/scrapy_baidu/`、`爬虫/scrapy_dushu/`、`爬虫/scrapy_movie/`：三个 Scrapy demo 工程

## 2.1 脚本清单（建议按顺序学）

### A. urllib（理解底层 + 练基本功）

- `[[爬虫/urllib基本使用.py]]`：`urlopen`、`read().decode()`  
- `[[爬虫/urllib 1个类型和6个方法.py]]`：`HTTPResponse`、`getcode/geturl/getheaders/readlines`  
- `[[爬虫/urllib.get请求的urlencode方法.py]]`：`urlencode` 组装 query（注意脚本里有“常见坑”，见后文）  
- `[[爬虫/urllib.请求对象的定制与UA反爬.py]]`：`Request + headers(User-Agent)`、中文参数 `quote`  
- `[[爬虫/urllib.post请求.py]]`：POST 表单提交（示例为翻译接口，签名类接口易过时）  
- `[[爬虫/urllib_ajax的多次get请求.py]]`：AJAX GET 分页（示例：豆瓣 chart）  
- `[[爬虫/urllib_ajax的多页post请求.py]]`：AJAX POST 分页（示例：KFC 门店）  
- `[[爬虫/urllib_handler处理器.py]]`：`HTTPHandler` / `build_opener`  
- `[[爬虫/urllib_代理.py]]`、`[[爬虫/urllib_代理池.py]]`：代理与代理池（入门思路）  
- `[[爬虫/urllib_异常.py]]`：`HTTPError/URLError` 基础异常处理  
- `[[爬虫/urllib_qq空间的cookie登录.py]]`：Cookie 登录（示例含敏感 cookie，建议本地自用/不要传播）  
- `[[爬虫/urllib 下载.py]]`：`urlretrieve` 下载（网页/图片/视频：其中“视频下载”示例更像占位）

### B. requests（更实用的 HTTP 客户端）

- `[[爬虫/requests_基本使用.py]]`：GET、状态码、响应头  
- `[[爬虫/requests_get请求.py]]`：GET + 参数（脚本里写法有坑，正确姿势见后文）  
- `[[爬虫/requests_post请求.py]]`：POST + `data` + JSON 解析  
- `[[爬虫/requests_代理.py]]`：requests 代理（同样存在“演示写法”和“推荐写法”的区别）  
- `[[爬虫/requests_cookie登陆.py]]`：表单登录 + 验证码图下载 + `Session` 保持会话（学习流程即可，账号/验证码/字段会变）

### C. 解析（把页面变成结构化数据）

- XPath（lxml）  
  - `[[爬虫/urllib_解析_xpath.py]]`：本地 HTML xpath 语法演示  
  - `[[爬虫/urllib_爬虫_解析_获取百度的百度一下.py]]`：从网页中提取按钮文字  
  - `[[爬虫/urllib_爬虫_解析_站长素材.py]]`：分页 + xpath 抓取资源并下载
- BeautifulSoup（bs4）  
  - `[[爬虫/bs4_解析.py]]`：`find/find_all/select`、节点/属性/文本  
  - `[[爬虫/bs4_ 爬麦当劳.py]]`：在线页面 + CSS 选择器提取文本
- JSON / JSONP / JsonPath  
  - `[[爬虫/jsonpath_解析.py]]`：`jsonpath` 常见表达式  
  - `[[爬虫/json_解析淘票票.py]]`：JSONP 去壳 → 保存为 JSON → jsonpath 提取字段  

### D. Selenium（动态页面与交互）

- `[[爬虫/selenium_基本使用.py]]`：打开页面 + `page_source`  
- `[[爬虫/selenium_元素定位.py]]`：常见定位方式（id/xpath/css/link text 等）  
- `[[爬虫/selenium_ 交互.py]]`：输入、点击、滚动、翻页、前进后退  
- `[[爬虫/selenium_获取元素信息.py]]`：`get_attribute/tag_name/text`  
- `[[爬虫/selenium_无界面浏览器.py]]`：Headless + 截图（注意脚本里 driver 路径是作者机器路径）

### E. Scrapy（工程化）

- `scrapy_baidu`：多个 spider + pipeline（含图片下载）  
  - spiders：`baidu.py`、`baidu_fanyi.py`、`dangdang.py`、`car.py`、`tongcheng.py`  
  - pipelines：写文件、下载当当图片  
- `scrapy_dushu`：`CrawlSpider + Rule + LinkExtractor` 抓连载分页  
- `scrapy_movie`：两段式抓取（列表页 → 详情页提取图片）

---

# 3. 环境准备（一次配好，后面顺畅）

## 3.1 建议的 Python/依赖（偏学习向）

建议用虚拟环境：

```bash
python -m venv .venv
.venv\\Scripts\\activate
pip install -U pip
pip install requests beautifulsoup4 lxml scrapy selenium jsonpath
```

> [!tip] 关于 `jsonpath`
> 本项目脚本是 `import jsonpath`，你本机如果装的库名不同（有的叫 `jsonpath-ng`），需要对齐一下；学习用的话装能跑的那个即可。

## 3.2 浏览器驱动（Selenium 必备）

- Chrome：`chromedriver`  
- Edge：`msedgedriver`

`[[爬虫/selenium_无界面浏览器.py]]` 里写死了 driver 路径（作者电脑路径），你需要改成自己机器上的驱动路径，或用系统 PATH 管理。

---

# 4. 抓取基础：urllib / requests（你真正要掌握的点）

下面不重复贴代码，而是把“要学会的能力”总结成清单，并配合仓库脚本作为例子。

## 4.1 你必须会的 8 件事

1. **会构造 URL**：路径、query 参数、编码（中文/特殊字符）  
2. **会带请求头**：尤其是 `User-Agent`，必要时 `Referer` / `Accept-Language`  
3. **会区分 GET/POST**：GET 用 query；POST 用 body（`application/x-www-form-urlencoded`）  
4. **会看响应**：状态码、响应头、编码、内容类型（HTML/JSON/图片）  
5. **会处理编码**：`response.encoding`（requests）或 `decode('utf-8')`（urllib）  
6. **会做异常处理**：超时、403/302、连接失败、解析失败  
7. **会做“温柔抓取”**：限速、重试、随机等待、并发控制  
8. **会保存结果**：HTML/JSON、图片、日志

这些点在脚本里都能找到对应：

- headers/UA：`[[爬虫/urllib.请求对象的定制与UA反爬.py]]`、`[[爬虫/requests_基本使用.py]]`
- 编码：`[[爬虫/requests_基本使用.py]]`（`response.encoding`）
- 异常：`[[爬虫/urllib_异常.py]]`
- 代理：`[[爬虫/urllib_代理.py]]`、`[[爬虫/requests_代理.py]]`

## 4.2 常见坑（本仓库里也踩到了）

### 坑 1：requests 的 GET 参数要用 `params`

`[[爬虫/requests_get请求.py]]` 里用了 `requests.get(url, data, headers=...)` 这种位置参数写法，**能跑但不推荐**，更容易传错。

推荐写法（记住就行）：

```python
requests.get(url, params={"wd": "北京"}, headers=headers, timeout=10)
```

### 坑 2：urllib 组好 query 但没用上

`[[爬虫/urllib.get请求的urlencode方法.py]]` 里把 `url = base_url + new_data` 组装出来了，但后面 `Request` 用的还是 `base_url`。  
学习时重点看思路：**`urlencode` → 拼接 → 用拼接后的 url 发请求**。

### 坑 3：`urlretrieve` 不能“下载视频网页”

`[[爬虫/urllib 下载.py]]` 里用 `urlretrieve` 指向的是西瓜视频的网页 URL（不是视频直链）。  
现代视频网站通常需要解析真实媒体地址（m3u8/mp4）并处理鉴权/防盗链；这里更适合“一笔带过”：把它当“下载 API 的演示”即可。

### 坑 4：Cookie/验证码/签名接口非常容易过时

- `[[爬虫/urllib_qq空间的cookie登录.py]]`：Cookie 会过期，也属于敏感信息  
- `[[爬虫/requests_cookie登陆.py]]`：验证码/隐藏字段会变，表单字段也会变  
- `[[爬虫/urllib.post请求.py]]`：带 `sign/mysticTime` 这类参数，多半是前端计算/加密的，**几乎必然过时**

学习建议：把它们当作“流程样板”——**先会抓静态/公开数据，再考虑登录态**。

---

# 5. 解析：XPath / bs4 / JSON（怎么选？怎么用？）

## 5.1 选型建议（一句话版）

- **JSON 接口优先**：稳定、结构化、解析简单  
- **XPath（lxml）**：速度快、表达能力强、适合写规则  
- **BeautifulSoup**：上手快，适合小项目/快速试验  
- **JsonPath**：对 JSON 做“路径查询”，适合快速取字段

## 5.2 XPath（lxml）要会的最小集合

结合 `[[爬虫/urllib_解析_xpath.py]]`：

- `//tag`：全局找  
- `@attr`：取属性  
- `text()`：取文本  
- `contains()` / `starts-with()`：模糊匹配  
- `and` / `|`：逻辑与、并集

练习建议：打开 `[[爬虫/urllib_解析_path.html]]`，自己写 5 条 xpath：

- 取所有城市名  
- 取 id=12 的文本  
- 取所有带 id 的 li  
- 取 class=c1 的 li 文本  
- 同时取 id=11 和 id=12（脚本最后那条）

## 5.3 BeautifulSoup（bs4）要会的最小集合

结合 `[[爬虫/bs4_解析.py]]`：

- `find/find_all`：按标签/属性查找  
- `select`：CSS 选择器（很实用）  
- `.attrs` / `.get_text()` / `.string`：属性与文本

练习建议：用 `[[爬虫/bs4_test.html]]`，写一条 `select` 拿到某个 id 的文本。

## 5.4 JSONP → JSON → JsonPath

`[[爬虫/json_解析淘票票.py]]` 提供了典型流程：

1. 请求拿到 JSONP（`jsonp109(...)`）  
2. 去掉外层函数壳  
3. 写入 `city.json`  
4. 用 `jsonpath` 抽取 `regionName`

这是很多老接口/前端接口常见的返回形式；核心能力是“**去壳**”和“**字段提取**”。

---

# 6. 反爬与稳定性：从“能跑”到“能用”

这里总结的是“学习阶段也值得养成”的工程习惯。

## 6.1 请求层面的基本策略

- **超时**：每次请求都加 `timeout`（requests）  
- **重试**：遇到临时失败（网络抖动、5xx）重试 2~3 次  
- **限速**：`sleep` 或 Scrapy 的 `DOWNLOAD_DELAY` / `AUTOTHROTTLE`  
- **请求头**：最少带 UA；必要时带 `Referer`  
- **会话**：登录态用 `requests.Session()` 或 Scrapy cookie middleware

## 6.2 代理：能不用就不用

`[[爬虫/urllib_代理.py]]`、`[[爬虫/urllib_代理池.py]]`、`[[爬虫/requests_代理.py]]` 给的是“怎么接入代理”的思路。  
现实里代理的坑更多：稳定性、速度、可用率、匿名度、成本、合规。学习阶段建议：

- 先把“**限速 + 头部 + 缓存**”做好  
- 需要代理时优先用**可信渠道**，并加“可用性检测/淘汰机制”

## 6.3 登录态：把敏感信息从代码里移出去

仓库里出现了 Cookie/账号/验证码示例。你在自己项目里建议：

- 用 `.env` 或系统环境变量存账号/密码/cookie  
- 写一个 `config.example.json` 做模板，不提交真实值  
- 不把 cookie 截图/复制到公开仓库

---

# 7. Selenium：什么时候该用？怎么用更稳？

## 7.1 什么时候用 Selenium

满足任一就考虑：

- 页面内容需要 JS 渲染（首屏 HTML 没数据）  
- 需要模拟交互（滚动加载、点击翻页、登录）  
- 接口被前端加密/签名且你不想逆向（学习阶段可以用浏览器“偷懒”）

仓库里常见动作都有演示：

- 定位：`[[爬虫/selenium_元素定位.py]]`
- 交互：`[[爬虫/selenium_ 交互.py]]`
- headless：`[[爬虫/selenium_无界面浏览器.py]]`

## 7.2 Selenium 稳定性技巧（学习阶段就值得做）

- 用 `WebDriverWait` 替代 `sleep`（仓库里 `duhsu.py/dushu2.py` 有 wait 的例子）  
- 尽量定位稳定的元素（id/name 优先，xpath 其次）  
- 页面跳转后重新找元素（避免 StaleElement）  
- 交互前先确认页面 ready/元素可点击

> [!note] 新技术提示（可选）
> 近年来很多团队从 Selenium 转向 **Playwright**（更适合现代网页/多浏览器/自动等待）。如果你后面要做更复杂的动态抓取，可以把 Playwright 作为下一阶段升级方向。

---

# 8. Scrapy：从脚本到“爬虫工程”

Scrapy 的核心是：**Spider 产出 Item / Request → Pipeline 处理 Item → Settings 控制全局行为**。

## 8.1 scrapy_baidu（多 spider + 管道 + 图片下载）

建议按这个顺序读：

1. `[[爬虫/scrapy_baidu/scrapy_baidu/spiders/dangdang.py]]`：分页抓取 + yield item  
2. `[[爬虫/scrapy_baidu/scrapy_baidu/items.py]]`：Item 定义  
3. `[[爬虫/scrapy_baidu/scrapy_baidu/pipelines.py]]`：写文件 + 下载图片  
4. `[[爬虫/scrapy_baidu/scrapy_baidu/settings.py]]`：启用 pipelines、编码、reactor

你能从这个 demo 学到的工程点：

- **多页抓取**：通过拼 url + `scrapy.Request(callback=...)`
- **管道写文件**：`open_spider/close_spider/process_item`
- **资源下载**：在 pipeline 里 `urlretrieve`（学习 OK；工程上更推荐 Scrapy 自带的 `ImagesPipeline`）

> [!warning] 输出格式建议
> `pipelines.py` 里是 `self.fp.write(str(item))`，这不是标准 JSON。学习阶段能看到结果即可；工程阶段建议用 Scrapy 的 Feed Export：
> - `scrapy crawl xxx -O output.json`
> - 或在 settings 里配置 `FEEDS`

## 8.2 scrapy_dushu（CrawlSpider + Rule）

`[[爬虫/scrapy_dushu/scrapy_dushu/spiders/read.py]]` 展示了：

- `CrawlSpider`：更“声明式”的爬取  
- `Rule + LinkExtractor(allow=...)`：只跟进匹配的分页链接  
- `parse_item`：在目标页面提取 item

## 8.3 scrapy_movie（两段式抓取）

`[[爬虫/scrapy_movie/scrapy_movie/spiders/mv.py]]` 展示了：

- 列表页提取详情页链接  
- `meta` 传参（把电影名带到详情页解析函数里）  
- 详情页再取图片地址

> [!note] 小瑕疵（不影响学习）
> `[[爬虫/scrapy_movie/scrapy_movie/pipelines.py]]` 里 `close_spider` 少了 `self` 参数（属于 demo 常见小问题）。学习时更关注“pipeline 生命周期”这个概念即可。

---

# 9. 实战练习清单（学完就能独立写一个小爬虫）

把练习做成 Obsidian 任务，完成一个勾一个：

- [ ] 用 requests 抓一个 JSON 接口，保存为 `data.json`
- [ ] 用 xpath 从一个列表页提取标题/链接
- [ ] 加上 `timeout + 重试 + 限速`，让脚本稳定跑 100 次不崩
- [ ] 把输出从 `print` 改为写入 CSV/JSON Lines
- [ ] 遇到动态页面，用 Selenium 抓 `page_source` 再解析
- [ ] 把同样任务迁移到 Scrapy：spider + item + pipeline
- [ ] 给 Scrapy 增加 `DOWNLOAD_DELAY` 和 `LOG_LEVEL`

---

# 10. 未来展望与“新技术探讨”（了解方向即可）

> 这部分变化快，我只给“方向”和“为什么”，不死记具体版本。

## 10.1 Async 化：更快但更难

- 同步 requests/urllib：简单直观  
- 异步抓取（`asyncio + httpx/aiohttp`）：吞吐更高，但要处理并发、限速、队列、异常传播

学习建议：**先把 Scrapy 用熟**，它本身就是事件驱动的；再考虑自己手写异步框架。

## 10.2 现代动态站点：Playwright + 抓接口

动态站点抓取的最佳路线经常是：

1) 用浏览器自动化把页面跑起来  
2) 打开 DevTools 找到真实数据接口（通常是 JSON）  
3) 回到 requests/Scrapy 直接抓接口（更快、更稳）

Playwright 在“自动等待/多上下文/网络拦截”上体验更好，是 Selenium 的强力替代选项。

## 10.3 结构化抽取：规则 +（可选）LLM

传统抽取靠 xpath/css 规则，优点是可控、成本低；缺点是页面一变就要维护。  
一些团队会把“复杂页面的字段抽取”交给 LLM（让模型把 HTML 转为结构化 JSON），优点是开发快，缺点是成本/稳定性/可解释性。

学习建议：先把规则抽取打牢，再把 LLM 当作“加速器”，而不是“救命稻草”。

## 10.4 数据管道与合规

未来更重要的是“爬下来之后怎么用”：

- 去重、增量更新、版本化  
- 数据质量（缺失/异常值/一致性）  
- 合规审计（数据来源、授权、用途、留存策略）  
- 任务调度（Airflow/Cron/K8s CronJob）与监控告警

---

# 11. 附录：运行指南（最短路径）

## 11.1 跑单文件脚本

在仓库根目录（也就是本文件同级）：

```bash
python .\\爬虫\\urllib基本使用.py
python .\\爬虫\\requests_基本使用.py
python .\\爬虫\\bs4_解析.py
```

## 11.2 跑 Scrapy spider

以 `scrapy_baidu` 为例：

```bash
cd .\\爬虫\\scrapy_baidu
scrapy list
scrapy crawl dangdang
```

输出文件会写到对应工程目录下（demo 里默认是 `books.json` / `movie.json` / `book.json` 这类）。

---

# 12. 你可以让我继续做的事（可选）

- 把这几个 Scrapy 工程的输出统一成标准 JSON/JSONL（替换 `str(item)`）  
- 把仓库里“疑似敏感信息（cookie/账号）”统一改成占位符 + `.env` 读取  
- 给每个脚本补一个“目标/输入/输出/依赖”的头部说明（更像课程笔记）

